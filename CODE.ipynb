{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkF9B6rbLJUr",
        "outputId": "8d7a2ed0-bb83-4137-80e6-436d7ac6d399"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~gboost'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "# Run once at top of notebook\n",
        "!pip install -q xgboost==1.7.6 pandas matplotlib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pveja1l7NeWE",
        "outputId": "c2005704-8e60-4bbe-8fe4-87414f6d658b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MCX_SILVER.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xhat\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ---- Load and basic cleanup ----\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m df_raw\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^0-9A-Za-z]+\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(c))\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_raw\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     33\u001b[0m date_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_raw\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c), df_raw\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\AJ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MCX_SILVER.csv'"
          ]
        }
      ],
      "source": [
        "# mcx_xgb_walkforward_light.py\n",
        "# Paste and run this in a Colab cell (after installing xgboost and uploading MCX_SILVER.csv to /content).\n",
        "\n",
        "import os, re, math, joblib, warnings\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
        "import xgboost as xgb\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_PATH = \"/content/MCX_SILVER.csv\"   # make sure file is uploaded here\n",
        "OUT_DIR = \"/content/mcx_xgb_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Kalman smoother ----\n",
        "def kalman_smooth(signal, q=1e-5, r=1e-2):\n",
        "    n = len(signal)\n",
        "    xhat = np.zeros(n); P = np.zeros(n)\n",
        "    xhat[0] = signal[0]; P[0] = 1.0\n",
        "    for k in range(1, n):\n",
        "        xhat_minus = xhat[k-1]\n",
        "        P_minus = P[k-1] + q\n",
        "        K_gain = P_minus / (P_minus + r + 1e-12)\n",
        "        xhat[k] = xhat_minus + K_gain * (signal[k] - xhat_minus)\n",
        "        P[k] = (1 - K_gain) * P_minus\n",
        "    return xhat\n",
        "\n",
        "# ---- Load and basic cleanup ----\n",
        "df_raw = pd.read_csv(DATA_PATH)\n",
        "df_raw.columns = [re.sub(r\"[^0-9A-Za-z]+\",\"_\", str(c)).lower() for c in df_raw.columns]\n",
        "date_col = next((c for c in df_raw.columns if \"date\" in c), df_raw.columns[0])\n",
        "close_col = next((c for c in df_raw.columns if \"close\" in c or \"settle\" in c or \"ltp\" in c), None)\n",
        "if close_col is None:\n",
        "    # fallback to numeric column with max variance\n",
        "    numeric_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
        "    close_col = numeric_cols[0] if numeric_cols else df_raw.columns[1]\n",
        "df = df_raw[[date_col, close_col]].rename(columns={date_col:\"Date\", close_col:\"Close\"})\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
        "df[\"Close\"] = pd.to_numeric(df[\"Close\"].astype(str).str.replace(r\"[^\\d\\.\\-eE]\",\"\", regex=True), errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"Date\",\"Close\"]).sort_values(\"Date\").reset_index(drop=True)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "\n",
        "# ---- Kalman smoothing ----\n",
        "df[\"Close_kf\"] = kalman_smooth(df[\"Close\"].values, q=1e-5, r=1e-2)\n",
        "\n",
        "# ---- Feature engineering (light) ----\n",
        "def engineer(df):\n",
        "    df = df.copy()\n",
        "    df[\"close\"] = df[\"Close_kf\"]\n",
        "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
        "    for w in (5,10,20,50):\n",
        "        df[f\"sma_{w}\"] = df[\"close\"].rolling(w).mean()\n",
        "        df[f\"ema_{w}\"] = df[\"close\"].ewm(span=w, adjust=False).mean()\n",
        "    df[\"roc_5\"] = df[\"close\"].pct_change(5)\n",
        "    for w in (5,20,60):\n",
        "        df[f\"vol_{w}\"] = df[\"ret_1\"].rolling(w).std()\n",
        "    delta = df[\"close\"].diff()\n",
        "    up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
        "    roll_up = up.ewm(com=13).mean(); roll_down = down.ewm(com=13).mean()\n",
        "    df[\"rsi_14\"] = 100 - 100/(1 + roll_up / (roll_down + 1e-12))\n",
        "    for lag in (1,2,3,5,10):\n",
        "        df[f\"ret_lag_{lag}\"] = df[\"ret_1\"].shift(lag)\n",
        "        df[f\"close_lag_{lag}\"] = df[\"close\"].shift(lag)\n",
        "    return df\n",
        "\n",
        "df_feat = engineer(df)\n",
        "df_feat = df_feat.dropna().reset_index(drop=True)\n",
        "df_feat[\"target_up\"] = (df_feat[\"close\"].shift(-1) > df_feat[\"close\"]).astype(int)\n",
        "df_feat = df_feat.dropna(subset=[\"target_up\"]).reset_index(drop=True)\n",
        "\n",
        "exclude = [\"Date\",\"Close\",\"Close_kf\",\"target_up\"]\n",
        "feature_cols = [c for c in df_feat.columns if c not in exclude and pd.api.types.is_numeric_dtype(df_feat[c])]\n",
        "X_all = df_feat[feature_cols].values\n",
        "y_all = df_feat[\"target_up\"].values\n",
        "dates_all = df_feat[\"Date\"].values\n",
        "print(\"Features:\", len(feature_cols), \"Samples:\", len(y_all))\n",
        "\n",
        "# ---- Last-step features for XGBoost ----\n",
        "SEQ_LEN = 60\n",
        "if len(X_all) <= SEQ_LEN + 10:\n",
        "    raise SystemExit(\"Not enough rows after feature engineering. Need > SEQ_LEN + 10 rows.\")\n",
        "X_last = np.array([X_all[i-1] for i in range(SEQ_LEN, len(X_all))])\n",
        "y_seq = y_all[SEQ_LEN:]\n",
        "dates_seq = dates_all[SEQ_LEN:]\n",
        "idxs = np.array([i for i in range(SEQ_LEN, len(X_all))])\n",
        "\n",
        "# ---- Walk-forward setup ----\n",
        "n = len(X_last)\n",
        "WF_FOLDS = 5\n",
        "test_block = max(int(n / (WF_FOLDS + 1)), 10)\n",
        "folds = []\n",
        "for f in range(WF_FOLDS):\n",
        "    train_end = test_block*(f+1)\n",
        "    test_start = train_end\n",
        "    test_end = min(train_end + test_block, n)\n",
        "    folds.append((0, train_end, test_start, test_end))\n",
        "folds = [t for t in folds if t[1]-t[0] > 50 and t[3]-t[2] > 5]\n",
        "print(\"Running folds:\", folds)\n",
        "\n",
        "summary = []\n",
        "for i,(tr0,train_end,test_start,test_end) in enumerate(folds):\n",
        "    X_train, y_train = X_last[tr0:train_end], y_seq[tr0:train_end]\n",
        "    X_test, y_test = X_last[test_start:test_end], y_seq[test_start:test_end]\n",
        "    dates_test = dates_seq[test_start:test_end]\n",
        "    if len(y_train) < 50 or len(y_test) < 10:\n",
        "        print(\"Skipping fold (too small)\"); continue\n",
        "    # scale\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_s = scaler.transform(X_train); X_test_s = scaler.transform(X_test)\n",
        "    # train XGBoost classifier\n",
        "    dtrain = xgb.DMatrix(X_train_s, label=y_train); dtest = xgb.DMatrix(X_test_s, label=y_test)\n",
        "    params = {\"objective\":\"binary:logistic\", \"eta\":0.05, \"max_depth\":4, \"eval_metric\":\"auc\"}\n",
        "    bst = xgb.train(params, dtrain, num_boost_round=100, verbose_eval=False)\n",
        "    y_prob = bst.predict(dtest); y_pred = (y_prob >= 0.5).astype(int)\n",
        "    # metrics\n",
        "    acc = accuracy_score(y_test, y_pred); f1 = f1_score(y_test, y_pred)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "    except:\n",
        "        auc = float(\"nan\")\n",
        "    # save model & scaler & metrics\n",
        "    fold_dir = os.path.join(OUT_DIR, f\"fold_{i+1}\"); os.makedirs(fold_dir, exist_ok=True)\n",
        "    bst.save_model(os.path.join(fold_dir, \"xgb_model.json\"))\n",
        "    joblib.dump(scaler, os.path.join(fold_dir, \"scaler.joblib\"))\n",
        "    joblib.dump({\"acc\":acc,\"f1\":f1,\"auc\":auc}, os.path.join(fold_dir, \"metrics.joblib\"))\n",
        "    # small, lightweight plots (try/except)\n",
        "    try:\n",
        "        plt.figure(figsize=(8,2)); plt.plot(dates_test, y_test, label=\"True\"); plt.plot(dates_test, y_prob, label=\"Pred prob\"); plt.legend(loc=\"upper right\"); plt.title(f\"Fold {i+1} True vs Pred Prob\"); plt.tight_layout(); plt.savefig(os.path.join(fold_dir,\"pred_prob_timeseries.png\")); plt.close()\n",
        "    except Exception as e:\n",
        "        print(\"Plot failed:\", e)\n",
        "    summary.append({\"fold\": i+1, \"n_train\": len(y_train), \"n_test\": len(y_test), \"acc\": acc, \"f1\": f1, \"auc\": auc})\n",
        "    print(f\"Fold {i+1} -> acc: {acc:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
        "\n",
        "summary_df = pd.DataFrame(summary)\n",
        "summary_df.to_csv(os.path.join(OUT_DIR, \"walkforward_summary.csv\"), index=False)\n",
        "print(\"\\nWalk-forward summary:\\n\", summary_df.to_string(index=False))\n",
        "print(\"\\nAll artifacts saved to:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOrc_YZaNjrQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGSZDyAwRhi1",
        "outputId": "92a33e17-75f6-4010-e342-64daa0d1ec28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded rows: 2511 Date range: 2015-12-11 to 2025-12-08\n",
            "Rows after features: 2451\n",
            "Features count: 25\n",
            "Total samples: 2450 Train: 1715 Stream-test: 735\n",
            "Training LSTM (this may take a moment)...\n",
            "LSTM trained and saved.\n",
            "ONLINE reg MSE: 5.68716743584474e-07 MAE: 0.000576132347857092\n",
            "ONLINE dir  Acc: 0.8884353741496599 F1: 0.917004048582996\n",
            "LSTM reg MSE: 2.44319544791732e-06 MAE: 0.001168247725375198\n",
            "Saved results & plots to /mnt/data/streaming_model_results\n",
            "Summary: {'online_reg_mse': 5.68716743584474e-07, 'online_reg_mae': 0.000576132347857092, 'online_dir_acc': 0.8884353741496599, 'online_dir_f1': 0.917004048582996, 'lstm_mse': 2.44319544791732e-06, 'lstm_mae': 0.001168247725375198, 'n_test': 735}\n"
          ]
        }
      ],
      "source": [
        "# streaming_trend_model.py\n",
        "# - Data path: /mnt/data/MCX_SILVER.csv\n",
        "# - Outputs -> /mnt/data/streaming_model_results/\n",
        "# Requirements: numpy, pandas, matplotlib, sklearn, tensorflow (optional for LSTM)\n",
        "# If TF not present, the script will skip the LSTM and still run online model simulation.\n",
        "\n",
        "import os, re, math, joblib, warnings\n",
        "from datetime import datetime\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "OUT = \"/mnt/data/streaming_model_results\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "DATA_PATH = \"/content/MCX_SILVER.csv\"\n",
        "\n",
        "# ---------- utilities ----------\n",
        "def safe_to_numeric(s):\n",
        "    if isinstance(s, str):\n",
        "        s2 = re.sub(r\"[^\\d\\.\\-eE]\",\"\", s)\n",
        "        try:\n",
        "            return float(s2)\n",
        "        except:\n",
        "            return np.nan\n",
        "    return s\n",
        "\n",
        "def kalman_smooth(signal, q=1e-5, r=1e-2):\n",
        "    n = len(signal)\n",
        "    xhat = np.zeros(n); P = np.zeros(n)\n",
        "    xhat[0] = signal[0]; P[0] = 1.0\n",
        "    for k in range(1,n):\n",
        "        xhat_minus = xhat[k-1]; P_minus = P[k-1] + q\n",
        "        K = P_minus / (P_minus + r + 1e-12)\n",
        "        xhat[k] = xhat_minus + K * (signal[k] - xhat_minus)\n",
        "        P[k] = (1 - K) * P_minus\n",
        "    return xhat\n",
        "\n",
        "# ---------- 1) Load & clean ----------\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Place your MCX CSV at: {DATA_PATH}\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "# normalize column names\n",
        "df.columns = [re.sub(r\"[^0-9A-Za-z]+\",\"_\", str(c)).lower() for c in df.columns]\n",
        "date_col = next((c for c in df.columns if \"date\" in c), df.columns[0])\n",
        "close_col = next((c for c in df.columns if \"close\" in c or \"ltp\" in c or \"settle\" in c), df.columns[1])\n",
        "df = df[[date_col, close_col]].rename(columns={date_col:\"Date\", close_col:\"Close\"})\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
        "df[\"Close\"] = df[\"Close\"].apply(safe_to_numeric)\n",
        "df = df.dropna(subset=[\"Date\",\"Close\"]).sort_values(\"Date\").reset_index(drop=True)\n",
        "print(\"Loaded rows:\", len(df), \"Date range:\", df['Date'].iloc[0].date(), \"to\", df['Date'].iloc[-1].date())\n",
        "\n",
        "# ---------- 2) Smooth and features ----------\n",
        "# Kalman smoothing (reduces micro-noise)\n",
        "df[\"close_kf\"] = kalman_smooth(df[\"Close\"].values, q=1e-5, r=1e-2)\n",
        "\n",
        "# Features: returns, log-returns, sma/ema, vol, rsi, lagged returns\n",
        "df[\"ret_1\"] = df[\"close_kf\"].pct_change()\n",
        "df[\"logret\"] = np.log(df[\"close_kf\"]).diff()\n",
        "for w in (5,10,20,50):\n",
        "    df[f\"sma_{w}\"] = df[\"close_kf\"].rolling(w).mean()\n",
        "    df[f\"ema_{w}\"] = df[\"close_kf\"].ewm(span=w, adjust=False).mean()\n",
        "df[\"roc_5\"] = df[\"close_kf\"].pct_change(5)\n",
        "for w in (5,20,60):\n",
        "    df[f\"vol_{w}\"] = df[\"ret_1\"].rolling(w).std()\n",
        "# RSI-ish\n",
        "delta = df[\"close_kf\"].diff()\n",
        "up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
        "roll_up = up.ewm(com=13).mean(); roll_down = down.ewm(com=13).mean()\n",
        "df[\"rsi_14\"] = 100 - 100/(1 + roll_up/(roll_down+1e-12))\n",
        "for lag in (1,2,3,5,10):\n",
        "    df[f\"ret_lag_{lag}\"] = df[\"ret_1\"].shift(lag)\n",
        "    df[f\"close_lag_{lag}\"] = df[\"close_kf\"].shift(lag)\n",
        "\n",
        "# Drop rows with NaNs from rolling windows\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "print(\"Rows after features:\", len(df))\n",
        "\n",
        "# ---------- 3) Targets ----------\n",
        "# Next-day return (magnitude) and direction label\n",
        "df[\"target_ret_1\"] = df[\"close_kf\"].shift(-1) / df[\"close_kf\"] - 1.0\n",
        "df[\"target_dir_1\"] = (df[\"target_ret_1\"] > 0).astype(int)\n",
        "# drop last row where target is NaN\n",
        "df = df.dropna(subset=[\"target_ret_1\",\"target_dir_1\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- 4) Feature matrix ----------\n",
        "exclude = [\"Date\",\"Close\",\"close_kf\",\"target_ret_1\",\"target_dir_1\"]\n",
        "feature_cols = [c for c in df.columns if c not in exclude and np.issubdtype(df[c].dtype, np.number)]\n",
        "X = df[feature_cols].values\n",
        "y_reg = df[\"target_ret_1\"].values  # regression target (float)\n",
        "y_clf = df[\"target_dir_1\"].values  # classification target (0/1)\n",
        "dates = df[\"Date\"].values\n",
        "print(\"Features count:\", len(feature_cols))\n",
        "\n",
        "# ---------- 5) Train/test time split ----------\n",
        "# Use initial 70% for training batch LSTM and initial online fit; remaining for streaming simulation\n",
        "n = len(X); train_n = int(n * 0.7)\n",
        "print(\"Total samples:\", n, \"Train:\", train_n, \"Stream-test:\", n-train_n)\n",
        "X_train, X_test = X[:train_n], X[train_n:]\n",
        "y_reg_train, y_reg_test = y_reg[:train_n], y_reg[train_n:]\n",
        "y_clf_train, y_clf_test = y_clf[:train_n], y_clf[train_n:]\n",
        "dates_train, dates_test = dates[:train_n], dates[train_n:]\n",
        "\n",
        "# Scale features: fit on train\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "joblib.dump(scaler, os.path.join(OUT, \"scaler.joblib\"))\n",
        "\n",
        "# ---------- 6) Build models ----------\n",
        "# 6A: Online models (fast, adapt using partial_fit)\n",
        "# For regression (magnitude)\n",
        "online_reg = SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "# For classification (direction)\n",
        "online_clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "\n",
        "# Initialize online models with a small subset (partial_fit requires classes for classifier)\n",
        "init_n = min(50, len(X_train_s))\n",
        "online_reg.partial_fit(X_train_s[:init_n], y_reg_train[:init_n])\n",
        "online_clf.partial_fit(X_train_s[:init_n], y_clf_train[:init_n], classes=np.array([0,1]))\n",
        "\n",
        "# 6B: Batch model (LSTM) for regression (optional - will be skipped if TensorFlow not available)\n",
        "use_lstm = True\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "    tf.random.set_seed(42)\n",
        "except Exception as e:\n",
        "    print(\"TensorFlow not available or failed to import; skipping LSTM batch model. Error:\", e)\n",
        "    use_lstm = False\n",
        "\n",
        "lstm_model = None\n",
        "if use_lstm:\n",
        "    # Prepare sequences for LSTM: use SEQ_LEN lookback\n",
        "    SEQ_LEN = 30\n",
        "    def make_sequences(Xa, seq_len):\n",
        "        Xs = []\n",
        "        for i in range(seq_len, len(Xa)):\n",
        "            Xs.append(Xa[i-seq_len:i])\n",
        "        return np.array(Xs)\n",
        "    # Build sequence arrays aligned to regression y (target at i -> sequence ends at i)\n",
        "    Xs_all = make_sequences(scaler.transform(X), SEQ_LEN)\n",
        "    y_reg_seq = y_reg[SEQ_LEN:]\n",
        "    # Train/test split accordingly\n",
        "    seq_train_n = int(len(Xs_all) * 0.7)\n",
        "    Xs_train, Xs_test = Xs_all[:seq_train_n], Xs_all[seq_train_n:]\n",
        "    yseq_train, yseq_test = y_reg_seq[:seq_train_n], y_reg_seq[seq_train_n:]\n",
        "    # model\n",
        "    inp = layers.Input(shape=(SEQ_LEN, Xs_train.shape[2]))\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(inp)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    out = layers.Dense(1, activation=\"linear\")(x)\n",
        "    lstm_model = models.Model(inp, out)\n",
        "    lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "    print(\"Training LSTM (this may take a moment)...\")\n",
        "    lstm_model.fit(Xs_train, yseq_train, validation_split=0.1, epochs=50, batch_size=32, callbacks=[es], verbose=0)\n",
        "    lstm_model.save(os.path.join(OUT, \"lstm_return_model.keras\"))\n",
        "    print(\"LSTM trained and saved.\")\n",
        "\n",
        "# ---------- 7) Streaming simulation over test set ----------\n",
        "results = []\n",
        "preds_online_reg = []\n",
        "preds_online_clf = []\n",
        "preds_lstm_reg = []\n",
        "\n",
        "# We'll step through test samples one-by-one (simulate streaming). For each t:\n",
        "#  - predict using online models (and optionally LSTM)\n",
        "#  - record predictions\n",
        "#  - then update online models using the true value at t (partial_fit)\n",
        "# For LSTM we use the pre-trained model only (no online updates)\n",
        "for i in range(len(X_test_s)):\n",
        "    x_t = X_test_s[i].reshape(1,-1)\n",
        "    # online predictions\n",
        "    pred_mag_online = online_reg.predict(x_t)[0]\n",
        "    pred_dir_online = online_clf.predict(x_t)[0]\n",
        "    preds_online_reg.append(pred_mag_online)\n",
        "    preds_online_clf.append(pred_dir_online)\n",
        "    # LSTM prediction: need to build sequence ending here in full series index\n",
        "    if use_lstm:\n",
        "        # find absolute index in full X_s (train_n offset)\n",
        "        abs_idx = train_n + i\n",
        "        if abs_idx - SEQ_LEN + 1 >= 0:\n",
        "            seq = scaler.transform(X)[abs_idx-SEQ_LEN+1:abs_idx+1]  # shape (SEQ_LEN, nfeat)\n",
        "            if seq.shape[0] == SEQ_LEN:\n",
        "                seq = seq.reshape(1, SEQ_LEN, seq.shape[1])\n",
        "                pred_l = float(lstm_model.predict(seq, verbose=0)[0,0])\n",
        "            else:\n",
        "                pred_l = np.nan\n",
        "        else:\n",
        "            pred_l = np.nan\n",
        "        preds_lstm_reg.append(pred_l)\n",
        "    # record actual\n",
        "    true_mag = y_reg_test[i]\n",
        "    true_dir = y_clf_test[i]\n",
        "    results.append({\n",
        "        \"date\": dates_test[i], \"true_mag\": true_mag, \"true_dir\": int(true_dir),\n",
        "        \"pred_online_mag\": pred_mag_online, \"pred_online_dir\": int(pred_dir_online),\n",
        "        \"pred_lstm_mag\": preds_lstm_reg[-1] if use_lstm else np.nan\n",
        "    })\n",
        "    # now update online models with the true label (simulate learning from new tick)\n",
        "    online_reg.partial_fit(x_t, np.array([true_mag]))\n",
        "    online_clf.partial_fit(x_t, np.array([true_dir]))\n",
        "\n",
        "# ---------- 8) Evaluate ----------\n",
        "res_df = pd.DataFrame(results)\n",
        "# Regression metrics (magnitude)\n",
        "mse_online = mean_squared_error(res_df[\"true_mag\"], res_df[\"pred_online_mag\"])\n",
        "mae_online = mean_absolute_error(res_df[\"true_mag\"], res_df[\"pred_online_mag\"])\n",
        "# LSTM metrics (if used)\n",
        "if use_lstm and res_df[\"pred_lstm_mag\"].notna().sum() > 0:\n",
        "    valid_mask = ~np.isnan(res_df[\"pred_lstm_mag\"])\n",
        "    mse_lstm = mean_squared_error(res_df[\"true_mag\"][valid_mask], res_df[\"pred_lstm_mag\"][valid_mask])\n",
        "    mae_lstm = mean_absolute_error(res_df[\"true_mag\"][valid_mask], res_df[\"pred_lstm_mag\"][valid_mask])\n",
        "else:\n",
        "    mse_lstm = mae_lstm = np.nan\n",
        "\n",
        "# Classification accuracy\n",
        "acc_online = accuracy_score(res_df[\"true_dir\"], res_df[\"pred_online_dir\"])\n",
        "f1_online = f1_score(res_df[\"true_dir\"], res_df[\"pred_online_dir\"])\n",
        "\n",
        "print(\"ONLINE reg MSE:\", mse_online, \"MAE:\", mae_online)\n",
        "print(\"ONLINE dir  Acc:\", acc_online, \"F1:\", f1_online)\n",
        "print(\"LSTM reg MSE:\", mse_lstm, \"MAE:\", mae_lstm)\n",
        "\n",
        "# ---------- 9) Visuals ----------\n",
        "# 9A: scatter predicted vs true (online reg)\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.scatter(res_df[\"true_mag\"], res_df[\"pred_online_mag\"], alpha=0.4, s=8)\n",
        "plt.xlabel(\"True next-day return\"); plt.ylabel(\"Predicted (online reg)\")\n",
        "plt.title(\"Online reg: Pred vs True\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT, \"pred_vs_true_online.png\")); plt.close()\n",
        "\n",
        "# 9B: time series of true vs predicted returns (first 300 points)\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.plot(res_df[\"date\"][:300], res_df[\"true_mag\"][:300], label=\"true\", linewidth=1)\n",
        "plt.plot(res_df[\"date\"][:300], res_df[\"pred_online_mag\"][:300], label=\"online_pred\", linewidth=1)\n",
        "if use_lstm:\n",
        "    plt.plot(res_df[\"date\"][:300], res_df[\"pred_lstm_mag\"][:300], label=\"lstm_pred\", linewidth=1)\n",
        "plt.legend(); plt.title(\"True vs Predicted next-day returns (first 300 samples)\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"timeseries_preds.png\")); plt.close()\n",
        "\n",
        "# 9C: direction prediction accuracy over rolling window\n",
        "res_df[\"correct_online_dir\"] = (res_df[\"true_dir\"] == res_df[\"pred_online_dir\"]).astype(int)\n",
        "res_df[\"rolling_acc_online\"] = res_df[\"correct_online_dir\"].rolling(50, min_periods=1).mean()\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(res_df[\"date\"], res_df[\"rolling_acc_online\"], label=\"rolling_acc_online\")\n",
        "plt.axhline(0.5, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
        "plt.title(\"Rolling accuracy (online classifier)\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"rolling_acc_online.png\")); plt.close()\n",
        "\n",
        "# 9D: simple strategy cumulative returns (apply predicted buy signal to actual next-day returns)\n",
        "# Strategy: go long when model predicts up (pred_dir==1), else flat. Compare buy&hold.\n",
        "strat_online = (res_df[\"pred_online_dir\"].shift(0).fillna(0).astype(int) * res_df[\"true_mag\"]).fillna(0)\n",
        "cum_strat_online = np.cumprod(1 + strat_online) - 1\n",
        "cum_hold = np.cumprod(1 + res_df[\"true_mag\"]) - 1\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(res_df[\"date\"], cum_strat_online, label=\"online strategy\")\n",
        "plt.plot(res_df[\"date\"], cum_hold, label=\"buy & hold\")\n",
        "plt.legend(); plt.title(\"Cumulative returns: online strategy vs buy&hold\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"cum_returns_online.png\")); plt.close()\n",
        "\n",
        "# Save results to csv\n",
        "res_df.to_csv(os.path.join(OUT, \"streaming_sim_results.csv\"), index=False)\n",
        "print(\"Saved results & plots to\", OUT)\n",
        "\n",
        "# ---------- 10) Quick diagnostics summary ----------\n",
        "summary = {\n",
        "    \"online_reg_mse\": float(mse_online), \"online_reg_mae\": float(mae_online),\n",
        "    \"online_dir_acc\": float(acc_online), \"online_dir_f1\": float(f1_online),\n",
        "    \"lstm_mse\": float(mse_lstm) if not math.isnan(mse_lstm) else None,\n",
        "    \"lstm_mae\": float(mae_lstm) if not math.isnan(mae_lstm) else None,\n",
        "    \"n_test\": len(res_df)\n",
        "}\n",
        "joblib.dump(summary, os.path.join(OUT, \"summary.joblib\"))\n",
        "print(\"Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM1UN1EMXfLL",
        "outputId": "b867b92c-306e-428a-9319-9dff8a663504"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-08 10:15:53.820 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.821 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.831 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.835 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.839 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.841 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.846 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.847 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.849 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.851 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.852 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.854 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.854 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.877 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.877 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.878 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.879 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.879 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.883 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.884 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-08 10:15:53.884 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "# app.py - Streamlit app for streaming-style trend + magnitude predictions\n",
        "# Usage:\n",
        "#   pip install streamlit pandas numpy scikit-learn matplotlib joblib xgboost\n",
        "#   streamlit run app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, re, joblib\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "DEFAULT_PATHS = [\n",
        "    \"/mnt/data/streaming_model_results/streaming_sim_results.csv\",\n",
        "    \"/mnt/data/MCX_SILVER.csv\",\n",
        "    \"/content/MCX_SILVER.csv\"\n",
        "]\n",
        "\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Streaming Trend Predictor\")\n",
        "\n",
        "st.title(\"Streaming Trend Predictor â€” Direction & Magnitude\")\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def safe_to_numeric(s):\n",
        "    if isinstance(s, str):\n",
        "        s2 = re.sub(r\"[^\\d\\.\\-eE]\",\"\", s)\n",
        "        try:\n",
        "            return float(s2)\n",
        "        except:\n",
        "            return np.nan\n",
        "    return s\n",
        "\n",
        "def kalman_smooth(signal, q=1e-5, r=1e-2):\n",
        "    n = len(signal)\n",
        "    xhat = np.zeros(n); P = np.zeros(n)\n",
        "    xhat[0] = signal[0]; P[0] = 1.0\n",
        "    for k in range(1,n):\n",
        "        xhat_minus = xhat[k-1]; P_minus = P[k-1] + q\n",
        "        K = P_minus / (P_minus + r + 1e-12)\n",
        "        xhat[k] = xhat_minus + K * (signal[k] - xhat_minus)\n",
        "        P[k] = (1 - K) * P_minus\n",
        "    return xhat\n",
        "\n",
        "def default_load_csv():\n",
        "    for p in DEFAULT_PATHS:\n",
        "        if os.path.exists(p):\n",
        "            try:\n",
        "                df = pd.read_csv(p)\n",
        "                return df, p\n",
        "            except:\n",
        "                pass\n",
        "    return None, None\n",
        "\n",
        "def prepare_features(df):\n",
        "    # Normalize column names\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"[^0-9A-Za-z]+\",\"_\", str(c)).lower() for c in df.columns]\n",
        "    date_col = next((c for c in df.columns if \"date\" in c), df.columns[0])\n",
        "    close_col = next((c for c in df.columns if \"close\" in c or \"ltp\" in c or \"settle\" in c), None)\n",
        "    if close_col is None:\n",
        "        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "        close_col = numeric_cols[0] if numeric_cols else df.columns[1]\n",
        "    df = df[[date_col, close_col]].rename(columns={date_col:\"Date\", close_col:\"Close\"})\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
        "    df[\"Close\"] = df[\"Close\"].apply(safe_to_numeric)\n",
        "    df = df.dropna(subset=[\"Date\",\"Close\"]).sort_values(\"Date\").reset_index(drop=True)\n",
        "    # smoothing + features\n",
        "    df[\"close_kf\"] = kalman_smooth(df[\"Close\"].values, q=1e-5, r=1e-2)\n",
        "    df[\"ret_1\"] = df[\"close_kf\"].pct_change()\n",
        "    df[\"logret\"] = np.log(df[\"close_kf\"]).diff()\n",
        "    for w in (5,10,20,50):\n",
        "        df[f\"sma_{w}\"] = df[\"close_kf\"].rolling(w).mean()\n",
        "        df[f\"ema_{w}\"] = df[\"close_kf\"].ewm(span=w, adjust=False).mean()\n",
        "    df[\"roc_5\"] = df[\"close_kf\"].pct_change(5)\n",
        "    for w in (5,20,60):\n",
        "        df[f\"vol_{w}\"] = df[\"ret_1\"].rolling(w).std()\n",
        "    # RSI-ish\n",
        "    delta = df[\"close_kf\"].diff()\n",
        "    up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
        "    roll_up = up.ewm(com=13).mean(); roll_down = down.ewm(com=13).mean()\n",
        "    df[\"rsi_14\"] = 100 - 100/(1 + roll_up/(roll_down+1e-12))\n",
        "    for lag in (1,2,3,5,10):\n",
        "        df[f\"ret_lag_{lag}\"] = df[\"ret_1\"].shift(lag)\n",
        "        df[f\"close_lag_{lag}\"] = df[\"close_kf\"].shift(lag)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    # targets\n",
        "    df[\"target_ret_1\"] = df[\"close_kf\"].shift(-1) / df[\"close_kf\"] - 1.0\n",
        "    df[\"target_dir_1\"] = (df[\"target_ret_1\"] > 0).astype(int)\n",
        "    df = df.dropna(subset=[\"target_ret_1\", \"target_dir_1\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Sidebar - file + options\n",
        "# ---------------------------\n",
        "with st.sidebar:\n",
        "    st.header(\"Input & Options\")\n",
        "    uploaded = st.file_uploader(\"Upload MCX CSV (optional)\", type=[\"csv\"])\n",
        "    use_default = st.checkbox(\"Use default path if available (/mnt/data/...)\", value=True)\n",
        "    seq_len = st.slider(\"Sequence length for LSTM (if used)\", min_value=10, max_value=120, value=30)\n",
        "    train_frac = st.slider(\"Training fraction (initial batch)\", min_value=0.5, max_value=0.9, value=0.7)\n",
        "    use_lstm = st.checkbox(\"Attempt LSTM baseline (requires tensorflow)\", value=False)\n",
        "    run_button = st.button(\"Run streaming sim\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load data\n",
        "# ---------------------------\n",
        "df_raw = None; source = None\n",
        "if uploaded is not None:\n",
        "    try:\n",
        "        df_raw = pd.read_csv(uploaded)\n",
        "        source = \"uploaded file\"\n",
        "    except Exception as e:\n",
        "        st.error(\"Failed to read uploaded file: \" + str(e))\n",
        "elif use_default:\n",
        "    df_raw, source = default_load_csv()\n",
        "    if df_raw is None:\n",
        "        st.info(\"No default CSV found; please upload a CSV.\")\n",
        "else:\n",
        "    st.info(\"Upload a CSV or enable default path.\")\n",
        "\n",
        "if df_raw is not None:\n",
        "    st.success(f\"Loaded data from: {source}\")\n",
        "    st.write(\"Preview:\")\n",
        "    st.dataframe(df_raw.head(6))\n",
        "\n",
        "# ---------------------------\n",
        "# Main pipeline + streaming simulation\n",
        "# ---------------------------\n",
        "if df_raw is not None and run_button:\n",
        "    with st.spinner(\"Preparing features and running streaming simulation...\"):\n",
        "        df = prepare_features(df_raw)\n",
        "        st.write(f\"Prepared features. Date range: {df['Date'].iloc[0].date()} â†’ {df['Date'].iloc[-1].date()}\")\n",
        "        st.write(\"Sample after feature prep:\")\n",
        "        st.dataframe(df.head(5))\n",
        "\n",
        "        # feature matrix\n",
        "        exclude = [\"Date\",\"Close\",\"close_kf\",\"target_ret_1\",\"target_dir_1\"]\n",
        "        feature_cols = [c for c in df.columns if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        X = df[feature_cols].values\n",
        "        y_reg = df[\"target_ret_1\"].values\n",
        "        y_clf = df[\"target_dir_1\"].values\n",
        "        dates = df[\"Date\"].values\n",
        "\n",
        "        n = len(X)\n",
        "        train_n = int(n * train_frac)\n",
        "        X_train, X_test = X[:train_n], X[train_n:]\n",
        "        y_reg_train, y_reg_test = y_reg[:train_n], y_reg[train_n:]\n",
        "        y_clf_train, y_clf_test = y_clf[:train_n], y_clf[train_n:]\n",
        "        dates_train, dates_test = dates[:train_n], dates[train_n:]\n",
        "\n",
        "        scaler = StandardScaler().fit(X_train)\n",
        "        X_train_s = scaler.transform(X_train)\n",
        "        X_test_s = scaler.transform(X_test)\n",
        "\n",
        "        # Attempt to load pre-saved models (optional)\n",
        "        saved_dir_candidates = [\"/mnt/data/streaming_model_results\", \"/content/streaming_model_results\"]\n",
        "        saved_loaded = False\n",
        "        model_info = {\"online_reg\":None, \"online_clf\":None, \"scaler\":None, \"lstm\":None}\n",
        "        for d in saved_dir_candidates:\n",
        "            try:\n",
        "                if os.path.exists(os.path.join(d, \"scaler.joblib\")):\n",
        "                    model_info[\"scaler\"] = joblib.load(os.path.join(d, \"scaler.joblib\"))\n",
        "                if os.path.exists(os.path.join(d, \"lstm_return_model.keras\")) and use_lstm:\n",
        "                    import tensorflow as tf\n",
        "                    model_info[\"lstm\"] = tf.keras.models.load_model(os.path.join(d, \"lstm_return_model.keras\"))\n",
        "                # we won't try to load SGD models from disk in this generic app\n",
        "                saved_loaded = True\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Build online models and initialize\n",
        "        online_reg = SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "        online_clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "        init_n = min(50, len(X_train_s))\n",
        "        if init_n <= 2:\n",
        "            st.error(\"Not enough training rows to initialize online model. Increase data or reduce train fraction.\")\n",
        "        online_reg.partial_fit(X_train_s[:init_n], y_reg_train[:init_n])\n",
        "        online_clf.partial_fit(X_train_s[:init_n], y_clf_train[:init_n], classes=np.array([0,1]))\n",
        "\n",
        "        # If user requests LSTM baseline, try to train (may take time)\n",
        "        lstm_model = None\n",
        "        if use_lstm:\n",
        "            try:\n",
        "                import tensorflow as tf\n",
        "                from tensorflow.keras import layers, models, callbacks\n",
        "                tf.random.set_seed(42)\n",
        "                # prepare sequences\n",
        "                def make_sequences(Xa, seq_len):\n",
        "                    Xs = []\n",
        "                    for i in range(seq_len, len(Xa)):\n",
        "                        Xs.append(Xa[i-seq_len:i])\n",
        "                    return np.array(Xs)\n",
        "                # Build sequence arrays aligned to regression y\n",
        "                scaled_all = scaler.transform(X)\n",
        "                Xs_all = make_sequences(scaled_all, seq_len)\n",
        "                yseq_all = y_reg[seq_len:]\n",
        "                seq_train_n = int(len(Xs_all) * 0.7)\n",
        "                Xs_train = Xs_all[:seq_train_n]; yseq_train = yseq_all[:seq_train_n]\n",
        "                # small LSTM\n",
        "                inp = layers.Input(shape=(seq_len, Xs_train.shape[2]))\n",
        "                x = layers.Bidirectional(layers.LSTM(32, return_sequences=False))(inp)\n",
        "                x = layers.Dropout(0.2)(x)\n",
        "                x = layers.Dense(16, activation=\"relu\")(x)\n",
        "                out = layers.Dense(1, activation=\"linear\")(x)\n",
        "                lstm_model = models.Model(inp, out)\n",
        "                lstm_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "                es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "                lstm_model.fit(Xs_train, yseq_train, validation_split=0.1, epochs=20, batch_size=32, callbacks=[es], verbose=0)\n",
        "                st.success(\"LSTM baseline trained.\")\n",
        "            except Exception as e:\n",
        "                st.warning(\"LSTM training failed or tensorflow not available: \" + str(e))\n",
        "                lstm_model = None\n",
        "\n",
        "        # Streaming simulation over the test set\n",
        "        results = []\n",
        "        preds_online_reg = []\n",
        "        preds_online_clf = []\n",
        "        preds_lstm_reg = []\n",
        "\n",
        "        for i in range(len(X_test_s)):\n",
        "            x_t = X_test_s[i].reshape(1,-1)\n",
        "            pred_mag_online = online_reg.predict(x_t)[0]\n",
        "            pred_dir_online = online_clf.predict(x_t)[0]\n",
        "            preds_online_reg.append(pred_mag_online)\n",
        "            preds_online_clf.append(int(pred_dir_online))\n",
        "            # LSTM pred if available (sequence ending at absolute index)\n",
        "            if lstm_model is not None:\n",
        "                abs_idx = train_n + i\n",
        "                if abs_idx - seq_len + 1 >= 0:\n",
        "                    seq = scaler.transform(X)[abs_idx-seq_len+1:abs_idx+1]\n",
        "                    if seq.shape[0] == seq_len:\n",
        "                        seq = seq.reshape(1, seq_len, seq.shape[1])\n",
        "                        try:\n",
        "                            pred_l = float(lstm_model.predict(seq, verbose=0)[0,0])\n",
        "                        except:\n",
        "                            pred_l = np.nan\n",
        "                    else:\n",
        "                        pred_l = np.nan\n",
        "                else:\n",
        "                    pred_l = np.nan\n",
        "                preds_lstm_reg.append(pred_l)\n",
        "            true_mag = y_reg_test[i]; true_dir = int(y_clf_test[i])\n",
        "            results.append({\n",
        "                \"date\": dates_test[i], \"true_mag\": true_mag, \"true_dir\": true_dir,\n",
        "                \"pred_online_mag\": pred_mag_online, \"pred_online_dir\": int(pred_dir_online),\n",
        "                \"pred_lstm_mag\": preds_lstm_reg[-1] if lstm_model is not None else np.nan\n",
        "            })\n",
        "            # update online models with the true label (simulate learning)\n",
        "            online_reg.partial_fit(x_t, np.array([true_mag]))\n",
        "            online_clf.partial_fit(x_t, np.array([true_dir]))\n",
        "\n",
        "        res_df = pd.DataFrame(results)\n",
        "        # metrics\n",
        "        mse_online = mean_squared_error(res_df[\"true_mag\"], res_df[\"pred_online_mag\"])\n",
        "        mae_online = mean_absolute_error(res_df[\"true_mag\"], res_df[\"pred_online_mag\"])\n",
        "        acc_online = accuracy_score(res_df[\"true_dir\"], res_df[\"pred_online_dir\"])\n",
        "        f1_online = f1_score(res_df[\"true_dir\"], res_df[\"pred_online_dir\"])\n",
        "        if lstm_model is not None and res_df[\"pred_lstm_mag\"].notna().sum() > 0:\n",
        "            valid_mask = ~np.isnan(res_df[\"pred_lstm_mag\"])\n",
        "            mse_lstm = mean_squared_error(res_df[\"true_mag\"][valid_mask], res_df[\"pred_lstm_mag\"][valid_mask])\n",
        "            mae_lstm = mean_absolute_error(res_df[\"true_mag\"][valid_mask], res_df[\"pred_lstm_mag\"][valid_mask])\n",
        "        else:\n",
        "            mse_lstm = mae_lstm = np.nan\n",
        "\n",
        "        # Show summary table\n",
        "        st.subheader(\"Performance Summary (stream-test)\")\n",
        "        summary_table = pd.DataFrame([\n",
        "            {\"Model\":\"Online (SGDReg/SGDClf)\",\"MSE\":mse_online,\"MAE\":mae_online,\"Accuracy\":acc_online,\"F1\":f1_online},\n",
        "            {\"Model\":\"LSTM (batch)\",\"MSE\":mse_lstm,\"MAE\":mae_lstm,\"Accuracy\":None,\"F1\":None}\n",
        "        ])\n",
        "        st.dataframe(summary_table.style.format({\"MSE\":\"{:.3e}\", \"MAE\":\"{:.6f}\", \"Accuracy\":\"{:.4f}\", \"F1\":\"{:.4f}\"}))\n",
        "\n",
        "        # Latest prediction box\n",
        "        st.subheader(\"Latest Prediction\")\n",
        "        latest = res_df.iloc[-1]\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Date\", str(pd.to_datetime(latest[\"date\"]).date()))\n",
        "        col2.metric(\"Predicted Direction\", \"UP\" if latest[\"pred_online_dir\"]==1 else \"DOWN\")\n",
        "        col3.metric(\"Predicted Next-Day Return\", f\"{latest['pred_online_mag']*100:.3f}%\")\n",
        "\n",
        "        # Plots: pred vs true scatter, time series, rolling accuracy, cumulative returns\n",
        "        st.subheader(\"Diagnostics & Plots\")\n",
        "        # 1) scatter\n",
        "        fig, ax = plt.subplots(figsize=(5,4))\n",
        "        ax.scatter(res_df[\"true_mag\"], res_df[\"pred_online_mag\"], alpha=0.4, s=8)\n",
        "        ax.set_xlabel(\"True next-day return\"); ax.set_ylabel(\"Predicted (online)\")\n",
        "        ax.set_title(\"Predicted vs True (online reg)\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # 2) timeseries (last 400 points)\n",
        "        maxpts = min(400, len(res_df))\n",
        "        fig2, ax2 = plt.subplots(figsize=(10,3))\n",
        "        ax2.plot(pd.to_datetime(res_df[\"date\"][-maxpts:]), res_df[\"true_mag\"][-maxpts:], label=\"true\", linewidth=1)\n",
        "        ax2.plot(pd.to_datetime(res_df[\"date\"][-maxpts:]), res_df[\"pred_online_mag\"][-maxpts:], label=\"online_pred\", linewidth=1)\n",
        "        if lstm_model is not None:\n",
        "            ax2.plot(pd.to_datetime(res_df[\"date\"][-maxpts:]), res_df[\"pred_lstm_mag\"][-maxpts:], label=\"lstm_pred\", linewidth=1)\n",
        "        ax2.legend(); ax2.set_title(\"True vs Predictions (recent)\")\n",
        "        st.pyplot(fig2)\n",
        "\n",
        "        # 3) rolling accuracy for direction\n",
        "        res_df[\"correct_online_dir\"] = (res_df[\"true_dir\"] == res_df[\"pred_online_dir\"]).astype(int)\n",
        "        res_df[\"rolling_acc_online\"] = res_df[\"correct_online_dir\"].rolling(50, min_periods=1).mean()\n",
        "        fig3, ax3 = plt.subplots(figsize=(10,2))\n",
        "        ax3.plot(pd.to_datetime(res_df[\"date\"]), res_df[\"rolling_acc_online\"], label=\"rolling_acc_online\")\n",
        "        ax3.axhline(0.5, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
        "        ax3.set_ylim(0,1)\n",
        "        ax3.set_title(\"Rolling accuracy (window=50)\")\n",
        "        st.pyplot(fig3)\n",
        "\n",
        "        # 4) cumulative returns: strategy vs buy & hold\n",
        "        strat_online = (res_df[\"pred_online_dir\"].astype(int) * res_df[\"true_mag\"]).fillna(0)\n",
        "        cum_strat_online = np.cumprod(1 + strat_online) - 1\n",
        "        cum_hold = np.cumprod(1 + res_df[\"true_mag\"]) - 1\n",
        "        fig4, ax4 = plt.subplots(figsize=(10,3))\n",
        "        ax4.plot(pd.to_datetime(res_df[\"date\"]), cum_strat_online, label=\"online strategy\")\n",
        "        ax4.plot(pd.to_datetime(res_df[\"date\"]), cum_hold, label=\"buy & hold\")\n",
        "        ax4.legend(); ax4.set_title(\"Cumulative returns\")\n",
        "        st.pyplot(fig4)\n",
        "\n",
        "        # export results\n",
        "        out_dir = os.path.join(\".\", \"streamlit_results\")\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        res_df.to_csv(os.path.join(out_dir, \"stream_sim_results.csv\"), index=False)\n",
        "        joblib.dump({\"summary\": {\"mse_online\":mse_online, \"mae_online\":mae_online, \"acc_online\":acc_online, \"f1_online\":f1_online}}, os.path.join(out_dir, \"summary.joblib\"))\n",
        "        st.success(f\"Streaming sim finished. Results saved to {out_dir}/stream_sim_results.csv\")\n",
        "        st.info(\"Tip: Use transaction-cost thresholding before trading; only trade when predicted magnitude > threshold.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgv8OJgUX1Fg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
